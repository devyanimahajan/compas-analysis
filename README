In the criminal justice system, decision-makers often rely on AI risk-assessment tools, like COMPAS, to predict an individual’s likelihood of reoffending. However, a landmark ProPublica analysis (https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm) revealed that these algorithms produce disproportionately high false-positive rates for Black defendants who are labeled “high risk” more often despite not reoffending, and disproportionately high false-negative rates for White defendants who reoffend more frequently than predicted. This model aims to decrease this diagnosis bias. Data is sourced from this repository (https://github.com/propublica/compas-analysis) and a multi-layer perceptron model is utilised. This model fulfils part of a final project in a Machine Learning II course at UChicago, as a part of its MS in Applied Data Science program. See **final_mlp.ipyb** for the final model and code.


Low  High
                 +---------+
Didn't Reoffend  |____|____|
Reoffended       |    |    |
                 +---------+


This repository contains a Jupyter notebook and data for the ProPublica story "Machine Bias."

Story:
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing/

Methodology:
https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm/

Notebook (you'll probably want to follow along in the methodology):
https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb

Main Dataset:
compas.db - a sqlite3 database containing criminal history, jail and prison time, demographics and COMPAS risk scores for defendants from Broward County.

Other files as needed for the analysis.
